{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxZVhbHlXbkF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPrRaqoq7353"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def cutmix(batch, alpha):\n",
    "    data, targets = batch\n",
    "\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "\n",
    "    image_h, image_w = data.shape[2:]\n",
    "    cx = np.random.uniform(0, image_w)\n",
    "    cy = np.random.uniform(0, image_h)\n",
    "    w = image_w * np.sqrt(1 - lam)\n",
    "    h = image_h * np.sqrt(1 - lam)\n",
    "    x0 = int(np.round(max(cx - w / 2, 0)))\n",
    "    x1 = int(np.round(min(cx + w / 2, image_w)))\n",
    "    y0 = int(np.round(max(cy - h / 2, 0)))\n",
    "    y1 = int(np.round(min(cy + h / 2, image_h)))\n",
    "\n",
    "    data[:, :, y0:y1, x0:x1] = shuffled_data[:, :, y0:y1, x0:x1]\n",
    "    targets = (targets, shuffled_targets, lam)\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "class CutMixCollator:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = torch.utils.data.dataloader.default_collate(batch)\n",
    "        batch = cutmix(batch, self.alpha)\n",
    "        return batch\n",
    "\n",
    "\n",
    "class CutMixCriterion:\n",
    "    def __init__(self, reduction):\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=reduction)\n",
    "\n",
    "    def __call__(self, preds, targets):\n",
    "        targets1, targets2, lam = targets[0], targets[1], targets[2]\n",
    "        #print(f\"target-1: {targets1}\")\n",
    "        ##print(f\"target-2: {targets2}\")\n",
    "        #print(f\"target-3: {lam}\")\n",
    "        return lam * self.criterion(\n",
    "            preds, targets1) + (1 - lam) * self.criterion(preds, targets2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ef7-L37tcWyI"
   },
   "source": [
    "**Importing Car Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrE3AzwjcO2G",
    "outputId": "780801a4-5d02-4dfc-890d-4461bec16285"
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "#Import Stanford car dataset and split in to train and test sets \n",
    "train_dataset = datasets.StanfordCars(root='./data', split='train', transform=data_transforms[\"train\"], download=True)\n",
    "valid_dataset = datasets.StanfordCars(root='./data', split='test', transform=data_transforms[\"val\"], download=True)\n",
    "\n",
    "# Total number of respective entries in training and validatio dataset\n",
    "N_train = len(train_dataset)\n",
    "N_valid = len(valid_dataset)\n",
    "print(f\"Training set size: {N_train} images\")\n",
    "print(f\"Valdiation set size: {N_valid} images\")\n",
    "\n",
    "# Creating training and validarion loader of batch size 32 and 4 worker processes\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "cc = CutMixCollator(1.0)\n",
    "d_cc = torch.utils.data.dataloader.default_collate\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn =cc,pin_memory=True,\n",
    "        drop_last=True, )\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn =d_cc, pin_memory=True,\n",
    "        drop_last=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vCs0bFVisgNc",
    "outputId": "9b30d176-7377-4ec1-ff4a-51fdb7e32ade"
   },
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUc0PKGhcO34",
    "outputId": "073fc4a1-3ac3-4ca2-b7d2-3d07cd434b0d"
   },
   "outputs": [],
   "source": [
    "# Fetch the individual classes of car dataset\n",
    "class_names = train_dataset.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGcGiMwRcO6A"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtQifKVNjZ8P"
   },
   "source": [
    "**Data Visualizaton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "otCfZpsocO75",
    "outputId": "b57305f0-e7bf-4f20-ad4b-f5fc9b1bd73b"
   },
   "outputs": [],
   "source": [
    "# Fetch the random 32 images from the complete train loader comprising of 8144 images\n",
    "def show_grid(data, titles=None):\n",
    "    # Create image tensor\n",
    "    data = data.numpy().transpose((0, 2, 3, 1))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    data = std * data + mean\n",
    "    data = np.clip(data, 0, 1)\n",
    "    \n",
    "    plt.figure(figsize=(8*2, 4*2))\n",
    "\n",
    "    # fetching selected images\n",
    "    for i in range(32):\n",
    "        plt.subplot(4,8,i+1)\n",
    "        plt.imshow(data[i])\n",
    "        plt.axis(\"off\")\n",
    "        if titles is not None:\n",
    "            # Add image title\n",
    "            plt.title(titles[i],fontsize = 7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "            \n",
    "# Get a batch of training data and displaying it\n",
    "inputs, classes = next(iter(train_loader))\n",
    "titles = [class_names[x] for x in classes[1]]\n",
    "\n",
    "show_grid(inputs, titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV4bwoxT4cUY"
   },
   "source": [
    "**Testing/Training Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlyBY1Y-cO-A"
   },
   "outputs": [],
   "source": [
    "# Function TRAIN_EPOCH\n",
    "# Model dataset training and validation functions\n",
    "# Parameters\n",
    "# Model Selection: ResNet - 152\n",
    "# Train Loader\n",
    "# Optimizer: Adam\n",
    "# Criterion: Cross Entropy Loss\n",
    "# Epoch: 15\n",
    "# Device: Cuda\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, criterion, epoch, device):\n",
    "    \"\"\" Training a model for one epoch \"\"\"\n",
    "    \n",
    "    loss_list = []\n",
    "    criterion = CutMixCriterion(reduction = \"mean\")\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # using images and labels on GPU\n",
    "        images = images.to(device)\n",
    "        labels = labels\n",
    "\n",
    "        if isinstance(labels, (tuple, list)):\n",
    "            l1,l2,l3 = labels\n",
    "            labels = (l1.to(device),l2.to(device),l3)\n",
    "        else:\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass to get output/log odss function\n",
    "        outputs = model(images)\n",
    "         \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "         \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "         \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    mean_loss = np.mean(loss_list)\n",
    "    return mean_loss, loss_list\n",
    "\n",
    "# Function EVAL_MODEL\n",
    "# Parameters\n",
    "# Model Selection: ResNet - 152\n",
    "# Evaluation Loader\n",
    "# Criterion: Cross Entropy Loss\n",
    "# Device: Cuda\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, eval_loader, criterion, device):\n",
    "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # saving individual loss at each epoch\n",
    "    loss_list = []\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    for images, labels in eval_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = model(images)\n",
    "                 \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "            \n",
    "        # Get predictions from the maximum value\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += len( torch.where(preds==labels)[0] )\n",
    "        total += len(labels)\n",
    "                 \n",
    "    # Total correct predictions and loss\n",
    "    accuracy = correct / total * 100\n",
    "    loss = np.mean(loss_list)\n",
    "    \n",
    "    return accuracy, loss\n",
    "\n",
    "# Function TRAIN_MODEL\n",
    "# Parameters\n",
    "# Model Selection: ResNet - 152\n",
    "# Optimzer: Adam\n",
    "# LR Scheduler: STEPLR\n",
    "# Criterion: Cross Entropy Loss\n",
    "# Train Loader\n",
    "# Evaluation Loader\n",
    "# Number of Epochs: 15\n",
    "# Tensor Board: True\n",
    "\n",
    "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs, tboard=None, start_epoch=0):\n",
    "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
    "    \n",
    "    # loss (train and validation) and validation accuracy lists saving repective accuracies and losses for each epoch\n",
    "    train_loss = []\n",
    "    val_loss =  []\n",
    "    valid_acc = []\n",
    "    loss_iters = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # validation epoch\n",
    "        # important for dropout and batch norms\n",
    "        model.eval()  \n",
    "        accuracy, loss = eval_model(\n",
    "                    model=model, eval_loader=valid_loader,\n",
    "                    criterion=criterion, device=device\n",
    "            )\n",
    "        \n",
    "        valid_acc.append(accuracy)\n",
    "        val_loss.append(loss)\n",
    "\n",
    "        # writing validation accuracy and losses on tensor board writer for each epoch\n",
    "        writer.add_scalar(f'Accuracy/Valid', accuracy, global_step=epoch+start_epoch)\n",
    "        writer.add_scalar(f'Loss/Valid', loss, global_step=epoch+start_epoch)\n",
    "        \n",
    "        # training epoch\n",
    "        model.train()  # important for dropout and batch norms\n",
    "        mean_loss, cur_loss_iters = train_epoch(\n",
    "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
    "                criterion=criterion, epoch=epoch, device=device\n",
    "            )\n",
    "\n",
    "        # decays the learning rate of each parameter group by gamma for every stepsize epoch \n",
    "        scheduler.step()\n",
    "        train_loss.append(mean_loss)\n",
    "        # write losses on tensor board writer\n",
    "        writer.add_scalar(f'Loss/Train', mean_loss, global_step=epoch+start_epoch)\n",
    "\n",
    "        loss_iters = loss_iters + cur_loss_iters\n",
    "        \n",
    "        # print the accuracies and losses for every 5 epochs\n",
    "        if(epoch % 5 == 0 or epoch==num_epochs-1):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
    "            print(f\"    Valid loss: {round(loss, 5)}\")\n",
    "            print(f\"    Accuracy: {accuracy}%\")\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    print(f\"Training completed\")\n",
    "    return train_loss, val_loss, loss_iters, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_puOcoScPAA"
   },
   "outputs": [],
   "source": [
    "# save the state of the model\n",
    "def save_model(model, optimizer, epoch, stats):\n",
    "    \"\"\" Saving model checkpoint \"\"\"\n",
    "    \n",
    "    if(not os.path.exists(\"models\")):\n",
    "        os.makedirs(\"models\")\n",
    "    savepath = f\"models/checkpoint_epoch_{epoch}.pth\"\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'stats': stats\n",
    "    }, savepath)\n",
    "    return\n",
    "\n",
    "# load saved/existing model\n",
    "def load_model(model, optimizer, savepath):\n",
    "    \"\"\" Loading pretrained checkpoint \"\"\"\n",
    "    \n",
    "    checkpoint = torch.load(savepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    stats = checkpoint[\"stats\"]\n",
    "    \n",
    "    return model, optimizer, epoch, stats\n",
    "\n",
    "# curved smoothening function\n",
    "def smooth(f, K=5):\n",
    "    \"\"\" Smoothing a function using a low-pass filter (mean) of size K \"\"\"\n",
    "    kernel = np.ones(K) / K\n",
    "    f = np.concatenate([f[:int(K//2)], f, f[int(-K//2):]])  # to account for boundaries\n",
    "    smooth_f = np.convolve(f, kernel, mode=\"same\")\n",
    "    smooth_f = smooth_f[K//2: -K//2]  # removing boundary-fixes\n",
    "    return smooth_f\n",
    "\n",
    "# set the initial random seed\n",
    "def set_random_seed(random_seed=None):\n",
    "    \"\"\"\n",
    "    Using random seed for numpy and torch\n",
    "    \"\"\"\n",
    "    if(random_seed is None):\n",
    "        random_seed = 13\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    return\n",
    "\n",
    "# call random seed function\n",
    "set_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omJWekbU4kwY"
   },
   "source": [
    "**2.1. Fine-Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xxndra3HcPEP"
   },
   "outputs": [],
   "source": [
    "# using model resnet152 having 152 layers\n",
    "# downloading pretrained model\n",
    "model = models.resnet152(pretrained=True)  # https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJJ4iBTU5th4"
   },
   "outputs": [],
   "source": [
    "# starting frim 512 input features reducing it to 196 car labels (which is the required output features)\n",
    "model.fc = nn.Sequential(\n",
    "        nn.Linear(512,256),  #input_features: 512, output_features:256\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256,196) #input_features: 256, output_features:196\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSWtrilCcPMy"
   },
   "outputs": [],
   "source": [
    "# replacing classification head\n",
    "model = models.resnet152(pretrained=True)\n",
    "num_nuerons = model.fc.in_features\n",
    "model.fc = nn.Linear(num_nuerons, 196)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uwdvqoGcPPD"
   },
   "outputs": [],
   "source": [
    "criterion = CutMixCriterion(reduction='mean')\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHCSJy028ykb"
   },
   "outputs": [],
   "source": [
    "# Adding tensor board lofs for CNN tuned\n",
    "TBOARD_LOGS = os.path.join(os.getcwd(), \"tboard_logs\", \"CNN_tuned\")\n",
    "if not os.path.exists(TBOARD_LOGS):\n",
    "    os.makedirs(TBOARD_LOGS)\n",
    "\n",
    "shutil.rmtree(TBOARD_LOGS)\n",
    "writer = SummaryWriter(TBOARD_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xXzCYGME4yoO",
    "outputId": "f25f2a36-e4dc-4ec0-d1ed-829de21f084c"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bySUq3G4ysi",
    "outputId": "5f5e3e83-f119-4103-81d3-9bf019fee8fb"
   },
   "outputs": [],
   "source": [
    "# Training and validating resnet152 on the dataset\n",
    "train_loss, val_loss, loss_iters, valid_acc = train_model(\n",
    "        model=model, optimizer=optimizer, scheduler=scheduler, criterion=criterion,\n",
    "        train_loader=train_loader, valid_loader=valid_loader, num_epochs=15, tboard=writer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RPU0seH04yur"
   },
   "outputs": [],
   "source": [
    "# plots and visualization curves for training, loss curves and validation accuracy\n",
    "plt.style.use('seaborn')\n",
    "fig, ax = plt.subplots(1,3)\n",
    "fig.set_size_inches(24,5)\n",
    "\n",
    "smooth_loss = smooth(loss_iters, 31)\n",
    "ax[0].plot(loss_iters, c=\"blue\", label=\"Loss\", linewidth=3, alpha=0.5)\n",
    "ax[0].plot(smooth_loss, c=\"red\", label=\"Smoothed Loss\", linewidth=3, alpha=1)\n",
    "ax[0].legend(loc=\"best\")\n",
    "ax[0].set_xlabel(\"Iteration\")\n",
    "ax[0].set_ylabel(\"CE Loss\")\n",
    "ax[0].set_title(\"Training Progress\")\n",
    "\n",
    "epochs = np.arange(len(train_loss)) + 1\n",
    "ax[1].plot(epochs, train_loss, c=\"red\", label=\"Train Loss\", linewidth=3)\n",
    "ax[1].plot(epochs, val_loss, c=\"blue\", label=\"Valid Loss\", linewidth=3)\n",
    "ax[1].legend(loc=\"best\")\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"CE Loss\")\n",
    "ax[1].set_title(\"Loss Curves\")\n",
    "\n",
    "epochs = np.arange(len(val_loss)) + 1\n",
    "ax[2].plot(epochs, valid_acc, c=\"red\", label=\"Valid accuracy\", linewidth=3)\n",
    "ax[2].legend(loc=\"best\")\n",
    "ax[2].set_xlabel(\"Epochs\")\n",
    "ax[2].set_ylabel(\"Accuracy (%)\")\n",
    "ax[2].set_title(f\"Valdiation Accuracy (max={round(np.max(valid_acc),2)}% @ epoch {np.argmax(valid_acc)+1})\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyQaArQ099hT"
   },
   "source": [
    "A) The training progress shows the gradual decrease of cross entropy loass as the number of iterattion increased over the course of time.\n",
    "\n",
    "B) The loss curvess (train loss, validation loss) shows the similar pattern of cross entropy loss over the training progress.\n",
    "\n",
    "C)The validation accuracy on the other hand is increasing over the training progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jj9Dtghn4ywl"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir tboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8UDU9LiBHku"
   },
   "source": [
    "**2.2: Select a good training recipe: augmentations, optimizer, learning rate scheduling, classifier, loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFGh1MIJ5bqQ"
   },
   "outputs": [],
   "source": [
    "# Installing Optuna\n",
    "!pip install --quiet optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyp8nH2MAjm6"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "from optuna.trial import TrialState\n",
    "optuna.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OS9cm6QiHTKV"
   },
   "outputs": [],
   "source": [
    "# Defining Learning Rate scheduler class \n",
    "class LRScheduler():\n",
    "    \"\"\"\n",
    "    Learning rate scheduler. If the validation loss does not decrease for the \n",
    "    given number of patience epochs, then the learning rate will decrease by\n",
    "    by given factor.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, optimizer, patience=5, min_lr=1e-6, factor=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        new_lr = old_lr * factor\n",
    "        :param optimizer: the optimizer we are using\n",
    "        :param patience: how many epochs to wait before updating the lr\n",
    "        :param min_lr: least lr value to reduce to while updating\n",
    "        :param factor: factor by which the lr should be updated\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.factor = factor\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
    "                self.optimizer,\n",
    "                mode='min',\n",
    "                patience=self.patience,\n",
    "                factor=self.factor,\n",
    "                min_lr=self.min_lr,\n",
    "                verbose=True\n",
    "            )\n",
    "    def __call__(self, val_loss):\n",
    "        self.lr_scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-1t1hGFAl9R"
   },
   "outputs": [],
   "source": [
    "# Objective Function for training and testing dataset to be used during hyperparameter optimization using optuna\n",
    "def objective(trial):\n",
    "    loss_list=[]\n",
    "    val_loss_list=[]\n",
    "\n",
    "    # Generate the model.\n",
    "    model = define_model(trial).to(device)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"]) #for hp tuning\n",
    "\n",
    "    LR = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True) #for hp tuning\n",
    "    \n",
    "    optimizer1 = getattr(optim, optimizer_name)(model.parameters(), lr=LR)\n",
    "    criterion = CutMixCriterion(reduction='mean')\n",
    "    lr_scheduler = LRScheduler(optimizer1)\n",
    "   \n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        # progress_bar = tqdm(train_loader, total=len(train_loader))\n",
    "        for i, (imgs, labels) in enumerate(train_loader):\n",
    "            # using GPU\n",
    "            imgs = imgs.to(device)\n",
    "            # forward pass\n",
    "            flattened_imgs = imgs.flatten(start_dim=1)\n",
    "            preds = model(flattened_imgs)\n",
    "            l1,l2,l3 = labels\n",
    "            labels = (l1.to(device),l2.to(device),l3)\n",
    "            # computing error\n",
    "            loss = criterion(preds, labels)\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # removing accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # backprogating error to compute gradients\n",
    "            loss.backward()\n",
    "        \n",
    "            # updating arameters\n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        # Validation of the model.\n",
    "        model.train()\n",
    "\n",
    "        n_correct = 0\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for i, (imgs, labels) in enumerate(valid_loader): \n",
    "                #everything needs to be on the same device\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "                # forward pass\n",
    "                flattened_imgs = imgs.flatten(start_dim=1)\n",
    "                preds = model(flattened_imgs)\n",
    "\n",
    "                loss = criterion(preds, labels)\n",
    "                val_loss_list.append(loss.item())\n",
    "                lr_scheduler(loss.item())\n",
    "                pred_labels = torch.argmax(preds, dim=-1)\n",
    "                cur_correct = len(torch.where(pred_labels == labels)[0])\n",
    "                n_correct = n_correct + cur_correct\n",
    "\n",
    "        accuracy = n_correct / len(valid_dataset)\n",
    "        # print(f\"Test accuracy: {round(accuracy,2)}%\")\n",
    "        trial.report(accuracy, epoch)\n",
    "        criterion = CutMixCriterion(reduction='mean')\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdzqkqmMAnqc"
   },
   "outputs": [],
   "source": [
    "# For optuna trials, we need to pass the parameters in such a way that these parameters maximizes the probability of getting good values.\n",
    "# We tested several values like n_layers between 1 to 5 and we found the optimal number should be somewhere around 1 to 3 as more layer make this this layer prone to overfitting as we don't have enough samples to \n",
    "# train as raw MLP (as opposed to a CNN which can extract image related features from the training dataset)\n",
    "\n",
    "CLASSES = 196\n",
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 2)\n",
    "    layers = []\n",
    "\n",
    "    in_features = 224 * 224 * 3\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 300,400)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, CLASSES))\n",
    "    # layers.append(nn.LogSoftmax(dim=1))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t18P5RFqApQx"
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcyqt0GjAq6b"
   },
   "outputs": [],
   "source": [
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(\"  Accuracy Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "polpIp0FAsmK"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRxl2pz1AxqL"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPU-ZljAAzjp"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_contour(study, params=['optimizer','lr','n_layers', 'n_units_l1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2peIuL6JAAP"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_edf(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brhzSeIQJAs6"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_intermediate_values(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmYrFbcaJBww"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WMtZDIWJBox"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwG3lsYFJBgy"
   },
   "outputs": [],
   "source": [
    "optuna.visualization.plot_pareto_front(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
